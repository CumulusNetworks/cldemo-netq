---
- hosts: network
  become: yes
  pre_tasks:
    # To prevent unexpected problems, define the minimum Cumulus Linux
    # Release supported by this demo.
    - name: Verify Minimum Software Version
      assert:
        that: "{{ansible_lsb.release | version_compare('3.5', '>=') }}"
        msg: "Cumulus Linux version must be 3.5 or later. Version {{ansible_lsb.release}} detected"

  tasks:
    # return code 0 means it's running
    # return code 1 means it's not in the output of grep
    # other return code is unknown
    - name: Check if NTP is already in the vrf
      shell: "vrf task list | grep ntpd"
      register: ntp_vrf
      failed_when: ntp_vrf.rc >= 2

    - name: Configure NTP in management VRF
      command: "{{item}}"
      with_items:
        - systemctl stop ntp.service
        - systemctl disable ntp.service
        - systemctl start ntp@mgmt
      when: ntp_vrf.rc != 0

    - name: Enable FRR Zebra
      lineinfile:
        dest: /etc/frr/daemons
        line: "zebra=yes"
        regexp: "zebra="
      notify: restart frr service

    - name: Enable FRR BGP
      lineinfile:
        dest: /etc/frr/daemons
        line: "bgpd=yes"
        regexp: "bgpd="
      notify: restart frr service

    - name: Copy Interfaces File
      copy:
        src: configurations/{{ansible_hostname}}/interfaces
        dest: /etc/network/interfaces
        validate: ifup -a -s -i %s
      notify: apply interface changes

    - name: Copy Routing Configuration
      copy:
        src: configurations/{{ansible_hostname}}/frr.conf
        dest: /etc/frr/frr.conf
        validate: vtysh -C -f %s
      notify: apply frr config

    # - name: Add Cumulus Repo
    #   apt_repository:
    #     repo: deb http://apps3.cumulusnetworks.com/repos/deb CumulusLinux-3 netq-1.2
    #     state: present

    # - name: Install NetQ
    #   apt:
    #     name: cumulus-netq
    #     update_cache: yes
    #   register: netq_installed

    # - name: Restart Rsyslog
    #   service:
    #     name: rsyslog
    #     state: restarted
    #   when: netq_installed.changed

    # - name: Check if NetQ is installed and running
    #   command: netq config status agent
    #   register: netq_running
    #   failed_when: netq_running.rc > 2

    # # Vx ships with NetQ running but not configured by default.
    # - name: Check if NetQ is installed but not running. 
    #   command: netq check agents
    #   register: netq_installed_not_running
    #   failed_when: netq_installed_not_running.rc > 2
      # when: netq_running.rc == 0 

    - name: Check if Netqd is running in the VRF
      shell: "vrf task list | grep netqd"
      register: netqd_vrf
      failed_when: netqd_vrf.rc >= 2

    - name: Restart Netqd
      command: "{{ item }}"
      with_items:
        - sudo systemctl stop netqd.service
        - sudo systemctl disable netqd.service
        - sudo systemctl enable netqd@mgmt.service
        - sudo systemctl start netqd@mgmt.service
      when: netqd_vrf.rc == 1

    - name: Add netq server IP
      command: netq config add server 192.168.0.254 vrf mgmt
      # when: netq_running.rc == 1 or netq_running.rc

    - name: Restart NetQ Agent
      command: netq config restart agent
      # when: netq_running.rc == 1

  handlers:
    - name: restart frr service
      service:
        name: frr
        state: restarted

    - name: apply interface changes
      service:
        name: networking
        state: reloaded

    - name: apply frr config
      service:
        name: frr
        state: reloaded

- hosts: servers
  become: yes
  tasks:
    - name: Copy Interface Config
      copy:
        src: configurations/{{ansible_hostname}}/interfaces
        dest: /etc/network/interfaces
      register: reboot

    - name: Correct LLDP Settings
      lineinfile:
        dest: /etc/lldpd.d/port_info.conf
        line: "configure lldp portidsubtype ifname"
        create: yes
      notify: restart LLDP

    - name: Apply LLDP Settings
      meta: flush_handlers

    - name: Install Docker-CE Repo Key
      apt_key:
        url: "https://download.docker.com/linux/ubuntu/gpg"
        state: present

    - name: Add Docker Repo
      apt_repository:
        repo: deb [arch=amd64] https://download.docker.com/linux/ubuntu xenial stable
        state: present
        update_cache: no

    - name: Add Cumulus Apt Key
      apt_key:
        url: "https://apps3.cumulusnetworks.com/setup/cumulus-apps-deb.pubkey"
        state: present

    - name: Add Cumulus Repo
      apt_repository:
        repo: deb https://apps3.cumulusnetworks.com/repos/deb xenial netq-1.2
        update_cache: no
        state: present

    - name: Install NetQ, DockerCE, ifupdown2, pip
      apt:
        name: "{{item}}"
        update_cache: yes
      with_items:
        - docker-ce
        - ifupdown2
        - cumulus-netq
        - python-pip

    - name: Install docker-py module
      pip:
        name: docker-py

    - name: Enable ifupdown2
      service:
        name: networking
        enabled: yes

    - name: Restart Rsyslog
      service:
        name: rsyslog
        state: restarted

    - name: Enable NetQ Service
      service:
        name: netqd
        enabled: yes
        state: started
      register: netq_started

    - name: Add netq server IP
      command: netq config add server 192.168.0.254

    - name: Restart NetQ Agent
      command: netq config restart agent

    - name: Add Docker settings to NetQ
      blockinfile:
        block: |
          docker:
            enable: true
            poll_period: 10
        dest: /etc/netq/netq.yml
      register: netq_docker

    - name: Apply NetQ Docker Settings
      command: netq config restart agent
      when: netq_docker.changed

    - name: Start Docker
      service:
        name: docker
        state: started
        enabled: yes

    - name: Enable ifupdown2 after reboot
      lineinfile:
        dest: "/lib/systemd/system/networking.service"
        regexp: '^RemainAfterExit'
        insertafter: '^\[Service\]'
        line: 'RemainAfterExit=yes'

    - name: Enable network interfaces
      command: ifup {{item}}
      ignore_errors: yes
      with_items:
        - eth1
        - eth2
        - lo

    - name: Create Quagga configuration directory
      file:
        path: /etc/quagga
        state: directory
        mode: 0775

    - name: Copy Quagga daemons File
      copy:
        src: configurations/{{ansible_hostname}}/daemons
        dest: /etc/quagga/daemons

    - name: Copy Quagga configuration File
      copy:
        src: configurations/{{ansible_hostname}}/quagga.conf
        dest: /etc/quagga/Quagga.conf

    - name: Deploy ROH Container
      docker_container:
        name: cumulus-roh
        privileged: true
        interactive: true
        network_mode: host
        restart_policy: unless-stopped
        tty: true
        recreate: yes
        image: cumulusnetworks/quagga:latest
        volumes:
          - "/etc/quagga/daemons:/etc/quagga/daemons"
          - "/etc/quagga/Quagga.conf:/etc/quagga/Quagga.conf"

  handlers:
      - name: restart LLDP
        service:
          name: lldpd
          state: restarted

# This section must break out the Docker Swarm Manager (server01)
# This is required to capture the worker token so it can be loaded as a var
# and then passed to the other swarm nodes
- hosts: server01
  become: yes
  tasks:

    # This determines if the docker swarm status is "active" or "inactive"
    # running at command line will likely give a "No swap limit support" warning
    # which requires a grub change and reboot to fix and doesn't matter for our usecase
    - name: Determine swarm status
      shell: >
        docker info | egrep '^Swarm: ' | cut -d ' ' -f2
      register: swarm_status

    - name: Configure Server01 as Docker Swarm manager
      command: docker swarm init --advertise-addr {{ansible_lo.ipv4_secondaries[0].address}}
      when: '"active" not in swarm_status.stdout_lines'

    - name: Get Worker Token
      command: docker swarm join-token -q worker
      register: swarm_manager_output

    - name: Set Worker Token as an Ansible fact
      set_fact:
        worker_token: "{{swarm_manager_output.stdout}}"


# This play will fail if server01 was not run first
- hosts: server02,server03,server04
  become: yes
  tasks:
    - name: Determine swarm status
      shell: >
        docker info | egrep '^Swarm: ' | cut -d ' ' -f2
      register: swarm_status

    - name: Join Docker Swarm
      command: docker swarm join --token {{hostvars['server01']['worker_token']}} --advertise-addr {{ansible_lo.ipv4_secondaries[0].address}} 10.0.0.31
      when: '"active" not in swarm_status.stdout_lines'

# Kick back to server01 to deploy the services
# Services can only be deployed from the master
# This is a little messy as a result of being in a single, flat playbook
- hosts: server01
  become: yes
  tasks:
    - name: Deploy Apache Containers
      command: docker service create --name apache_web --replicas 3 --publish 8080:80 php:5.6-apache
